{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ5iTmBRC8gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djNtGs9menyS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c25bd2ee-6ef4-4340-f93c-18a4bd8faa26"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "!unzip -q \"/content/deep-fake-detection-knu-2020.zip\"\n",
        "\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace submission_sample.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3SqRQrnWhpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a226f359-dc89-4d17-8dda-ae3e9b77d1a0"
      },
      "source": [
        "train_data_path = '/content/train/train/'\n",
        "\n",
        "VAL_SIZE = 0.15 # percentage of data for validation\n",
        "\n",
        "transform = transforms.Compose(\n",
        "        [transforms.Grayscale(num_output_channels=1),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Creating dataset for training and validation\n",
        "train_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=train_data_path,\n",
        "        transform=transform)\n",
        "\n",
        "# Shuffling data and choosing data that will be used for training and validation\n",
        "num_train = len(train_dataset)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(VAL_SIZE * num_train))\n",
        "train_idx, val_idx = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "# Creating dataloaders for training and validation\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size = BATCH_SIZE,\n",
        "        num_workers=0,\n",
        "        sampler=train_sampler)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size = BATCH_SIZE,\n",
        "        num_workers=0,\n",
        "        sampler=val_sampler)\n",
        "\n",
        "print(f\"Length train: {len(train_idx)}\")\n",
        "print(f\"Length valid: {len(val_idx)}\")\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length train: 102000\n",
            "Length valid: 18000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMXGqrTjGc7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "use_cuda = True\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_batch_size=64\n",
        "test_batch_size=1000\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_NOXTQ-GK6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
        "        self.bnm1 = nn.BatchNorm2d(20)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5)\n",
        "        self.bnm2 = nn.BatchNorm2d(50)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = self.bnm1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = self.bnm2(x)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5sZ6btaI3kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "lr=0.01\n",
        "momentum = 0.5\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDZo_fDcDOD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch,log_interval):\n",
        "    model.train()\n",
        "    avg_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad() # zero the gradient buffers\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step() # Does the update\n",
        "        avg_loss+=F.nll_loss(output, target, reduction='sum').item()\n",
        "        \n",
        "        if batch_idx % log_interval == 0:\n",
        "          print('Train Epoch: {} [{:5.0f}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\n",
        "    avg_loss/=len(train_loader.dataset)\n",
        "    return avg_loss\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            val_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
        "        val_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "    accuracy = 100. * correct / len(val_loader.dataset)\n",
        "    return val_loss,accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAEIpawOC_Wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 7\n",
        "log_interval = 47\n",
        "save_model = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU3Hq2VUjiKN",
        "colab_type": "code",
        "outputId": "b1c6e5ef-878d-4417-8eec-59234f60b5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "accuracy_list = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    trn_loss = train(model, device, train_loader, optimizer, epoch,log_interval)\n",
        "    test_loss,accuracy = test(model, device, val_loader)\n",
        "    train_losses.append(trn_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    accuracy_list.append(accuracy)\n",
        "    if epoch==3 or epoch==10 or epoch==7 or epoch==13 :\n",
        "      lr=lr/1.2\n",
        "      optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "if (save_model):\n",
        "    torch.save(model.state_dict(),\"mnist_cnn.pt\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [    0/120000 (0%)]\tLoss: 0.728152\n",
            "Train Epoch: 1 [ 3008/120000 (3%)]\tLoss: 0.635702\n",
            "Train Epoch: 1 [ 6016/120000 (6%)]\tLoss: 0.592979\n",
            "Train Epoch: 1 [ 9024/120000 (9%)]\tLoss: 0.620589\n",
            "Train Epoch: 1 [12032/120000 (12%)]\tLoss: 0.537447\n",
            "Train Epoch: 1 [15040/120000 (15%)]\tLoss: 0.425116\n",
            "Train Epoch: 1 [18048/120000 (18%)]\tLoss: 0.431650\n",
            "Train Epoch: 1 [21056/120000 (21%)]\tLoss: 0.514290\n",
            "Train Epoch: 1 [24064/120000 (24%)]\tLoss: 0.284387\n",
            "Train Epoch: 1 [27072/120000 (27%)]\tLoss: 0.323081\n",
            "Train Epoch: 1 [30080/120000 (29%)]\tLoss: 0.373391\n",
            "Train Epoch: 1 [33088/120000 (32%)]\tLoss: 0.387536\n",
            "Train Epoch: 1 [36096/120000 (35%)]\tLoss: 0.307200\n",
            "Train Epoch: 1 [39104/120000 (38%)]\tLoss: 0.270705\n",
            "Train Epoch: 1 [42112/120000 (41%)]\tLoss: 0.253173\n",
            "Train Epoch: 1 [45120/120000 (44%)]\tLoss: 0.185607\n",
            "Train Epoch: 1 [48128/120000 (47%)]\tLoss: 0.255567\n",
            "Train Epoch: 1 [51136/120000 (50%)]\tLoss: 0.184741\n",
            "Train Epoch: 1 [54144/120000 (53%)]\tLoss: 0.262995\n",
            "Train Epoch: 1 [57152/120000 (56%)]\tLoss: 0.183786\n",
            "Train Epoch: 1 [60160/120000 (59%)]\tLoss: 0.307041\n",
            "Train Epoch: 1 [63168/120000 (62%)]\tLoss: 0.255785\n",
            "Train Epoch: 1 [66176/120000 (65%)]\tLoss: 0.174736\n",
            "Train Epoch: 1 [69184/120000 (68%)]\tLoss: 0.190323\n",
            "Train Epoch: 1 [72192/120000 (71%)]\tLoss: 0.114730\n",
            "Train Epoch: 1 [75200/120000 (74%)]\tLoss: 0.125878\n",
            "Train Epoch: 1 [78208/120000 (77%)]\tLoss: 0.166760\n",
            "Train Epoch: 1 [81216/120000 (80%)]\tLoss: 0.094555\n",
            "Train Epoch: 1 [84224/120000 (83%)]\tLoss: 0.303513\n",
            "Train Epoch: 1 [87232/120000 (86%)]\tLoss: 0.191355\n",
            "Train Epoch: 1 [90240/120000 (88%)]\tLoss: 0.250126\n",
            "Train Epoch: 1 [93248/120000 (91%)]\tLoss: 0.082496\n",
            "Train Epoch: 1 [96256/120000 (94%)]\tLoss: 0.217311\n",
            "Train Epoch: 1 [99264/120000 (97%)]\tLoss: 0.177863\n",
            "\n",
            "Test set: Average loss: 0.0223, Accuracy: 16973/120000 (14.1%)\n",
            "\n",
            "Train Epoch: 2 [    0/120000 (0%)]\tLoss: 0.075427\n",
            "Train Epoch: 2 [ 3008/120000 (3%)]\tLoss: 0.176621\n",
            "Train Epoch: 2 [ 6016/120000 (6%)]\tLoss: 0.244698\n",
            "Train Epoch: 2 [ 9024/120000 (9%)]\tLoss: 0.087964\n",
            "Train Epoch: 2 [12032/120000 (12%)]\tLoss: 0.080483\n",
            "Train Epoch: 2 [15040/120000 (15%)]\tLoss: 0.237923\n",
            "Train Epoch: 2 [18048/120000 (18%)]\tLoss: 0.079848\n",
            "Train Epoch: 2 [21056/120000 (21%)]\tLoss: 0.058741\n",
            "Train Epoch: 2 [24064/120000 (24%)]\tLoss: 0.077656\n",
            "Train Epoch: 2 [27072/120000 (27%)]\tLoss: 0.220021\n",
            "Train Epoch: 2 [30080/120000 (29%)]\tLoss: 0.037887\n",
            "Train Epoch: 2 [33088/120000 (32%)]\tLoss: 0.152602\n",
            "Train Epoch: 2 [36096/120000 (35%)]\tLoss: 0.085860\n",
            "Train Epoch: 2 [39104/120000 (38%)]\tLoss: 0.097315\n",
            "Train Epoch: 2 [42112/120000 (41%)]\tLoss: 0.060898\n",
            "Train Epoch: 2 [45120/120000 (44%)]\tLoss: 0.123490\n",
            "Train Epoch: 2 [48128/120000 (47%)]\tLoss: 0.106606\n",
            "Train Epoch: 2 [51136/120000 (50%)]\tLoss: 0.049145\n",
            "Train Epoch: 2 [54144/120000 (53%)]\tLoss: 0.157246\n",
            "Train Epoch: 2 [57152/120000 (56%)]\tLoss: 0.139498\n",
            "Train Epoch: 2 [60160/120000 (59%)]\tLoss: 0.105844\n",
            "Train Epoch: 2 [63168/120000 (62%)]\tLoss: 0.109594\n",
            "Train Epoch: 2 [66176/120000 (65%)]\tLoss: 0.068762\n",
            "Train Epoch: 2 [69184/120000 (68%)]\tLoss: 0.108514\n",
            "Train Epoch: 2 [72192/120000 (71%)]\tLoss: 0.091188\n",
            "Train Epoch: 2 [75200/120000 (74%)]\tLoss: 0.050470\n",
            "Train Epoch: 2 [78208/120000 (77%)]\tLoss: 0.023784\n",
            "Train Epoch: 2 [81216/120000 (80%)]\tLoss: 0.055441\n",
            "Train Epoch: 2 [84224/120000 (83%)]\tLoss: 0.093537\n",
            "Train Epoch: 2 [87232/120000 (86%)]\tLoss: 0.080667\n",
            "Train Epoch: 2 [90240/120000 (88%)]\tLoss: 0.039108\n",
            "Train Epoch: 2 [93248/120000 (91%)]\tLoss: 0.058998\n",
            "Train Epoch: 2 [96256/120000 (94%)]\tLoss: 0.097178\n",
            "Train Epoch: 2 [99264/120000 (97%)]\tLoss: 0.085027\n",
            "\n",
            "Test set: Average loss: 0.0140, Accuracy: 17385/120000 (14.5%)\n",
            "\n",
            "Train Epoch: 3 [    0/120000 (0%)]\tLoss: 0.060377\n",
            "Train Epoch: 3 [ 3008/120000 (3%)]\tLoss: 0.047474\n",
            "Train Epoch: 3 [ 6016/120000 (6%)]\tLoss: 0.077450\n",
            "Train Epoch: 3 [ 9024/120000 (9%)]\tLoss: 0.014933\n",
            "Train Epoch: 3 [12032/120000 (12%)]\tLoss: 0.059244\n",
            "Train Epoch: 3 [15040/120000 (15%)]\tLoss: 0.043544\n",
            "Train Epoch: 3 [18048/120000 (18%)]\tLoss: 0.022943\n",
            "Train Epoch: 3 [21056/120000 (21%)]\tLoss: 0.092560\n",
            "Train Epoch: 3 [24064/120000 (24%)]\tLoss: 0.047343\n",
            "Train Epoch: 3 [27072/120000 (27%)]\tLoss: 0.082038\n",
            "Train Epoch: 3 [30080/120000 (29%)]\tLoss: 0.064568\n",
            "Train Epoch: 3 [33088/120000 (32%)]\tLoss: 0.114634\n",
            "Train Epoch: 3 [36096/120000 (35%)]\tLoss: 0.070444\n",
            "Train Epoch: 3 [39104/120000 (38%)]\tLoss: 0.065802\n",
            "Train Epoch: 3 [42112/120000 (41%)]\tLoss: 0.023613\n",
            "Train Epoch: 3 [45120/120000 (44%)]\tLoss: 0.024110\n",
            "Train Epoch: 3 [48128/120000 (47%)]\tLoss: 0.074740\n",
            "Train Epoch: 3 [51136/120000 (50%)]\tLoss: 0.076047\n",
            "Train Epoch: 3 [54144/120000 (53%)]\tLoss: 0.016129\n",
            "Train Epoch: 3 [57152/120000 (56%)]\tLoss: 0.062974\n",
            "Train Epoch: 3 [60160/120000 (59%)]\tLoss: 0.066211\n",
            "Train Epoch: 3 [63168/120000 (62%)]\tLoss: 0.056988\n",
            "Train Epoch: 3 [66176/120000 (65%)]\tLoss: 0.015155\n",
            "Train Epoch: 3 [69184/120000 (68%)]\tLoss: 0.014928\n",
            "Train Epoch: 3 [72192/120000 (71%)]\tLoss: 0.048347\n",
            "Train Epoch: 3 [75200/120000 (74%)]\tLoss: 0.025301\n",
            "Train Epoch: 3 [78208/120000 (77%)]\tLoss: 0.014304\n",
            "Train Epoch: 3 [81216/120000 (80%)]\tLoss: 0.037157\n",
            "Train Epoch: 3 [84224/120000 (83%)]\tLoss: 0.026912\n",
            "Train Epoch: 3 [87232/120000 (86%)]\tLoss: 0.108809\n",
            "Train Epoch: 3 [90240/120000 (88%)]\tLoss: 0.075360\n",
            "Train Epoch: 3 [93248/120000 (91%)]\tLoss: 0.168765\n",
            "Train Epoch: 3 [96256/120000 (94%)]\tLoss: 0.028410\n",
            "Train Epoch: 3 [99264/120000 (97%)]\tLoss: 0.008517\n",
            "\n",
            "Test set: Average loss: 0.0104, Accuracy: 17542/120000 (14.6%)\n",
            "\n",
            "Train Epoch: 4 [    0/120000 (0%)]\tLoss: 0.041325\n",
            "Train Epoch: 4 [ 3008/120000 (3%)]\tLoss: 0.012529\n",
            "Train Epoch: 4 [ 6016/120000 (6%)]\tLoss: 0.029734\n",
            "Train Epoch: 4 [ 9024/120000 (9%)]\tLoss: 0.026850\n",
            "Train Epoch: 4 [12032/120000 (12%)]\tLoss: 0.046199\n",
            "Train Epoch: 4 [15040/120000 (15%)]\tLoss: 0.018320\n",
            "Train Epoch: 4 [18048/120000 (18%)]\tLoss: 0.026846\n",
            "Train Epoch: 4 [21056/120000 (21%)]\tLoss: 0.026872\n",
            "Train Epoch: 4 [24064/120000 (24%)]\tLoss: 0.021160\n",
            "Train Epoch: 4 [27072/120000 (27%)]\tLoss: 0.025677\n",
            "Train Epoch: 4 [30080/120000 (29%)]\tLoss: 0.011304\n",
            "Train Epoch: 4 [33088/120000 (32%)]\tLoss: 0.018551\n",
            "Train Epoch: 4 [36096/120000 (35%)]\tLoss: 0.065472\n",
            "Train Epoch: 4 [39104/120000 (38%)]\tLoss: 0.017263\n",
            "Train Epoch: 4 [42112/120000 (41%)]\tLoss: 0.055857\n",
            "Train Epoch: 4 [45120/120000 (44%)]\tLoss: 0.012025\n",
            "Train Epoch: 4 [48128/120000 (47%)]\tLoss: 0.008546\n",
            "Train Epoch: 4 [51136/120000 (50%)]\tLoss: 0.098836\n",
            "Train Epoch: 4 [54144/120000 (53%)]\tLoss: 0.029889\n",
            "Train Epoch: 4 [57152/120000 (56%)]\tLoss: 0.021608\n",
            "Train Epoch: 4 [60160/120000 (59%)]\tLoss: 0.037494\n",
            "Train Epoch: 4 [63168/120000 (62%)]\tLoss: 0.044124\n",
            "Train Epoch: 4 [66176/120000 (65%)]\tLoss: 0.008140\n",
            "Train Epoch: 4 [69184/120000 (68%)]\tLoss: 0.034953\n",
            "Train Epoch: 4 [72192/120000 (71%)]\tLoss: 0.020559\n",
            "Train Epoch: 4 [75200/120000 (74%)]\tLoss: 0.038890\n",
            "Train Epoch: 4 [78208/120000 (77%)]\tLoss: 0.030033\n",
            "Train Epoch: 4 [81216/120000 (80%)]\tLoss: 0.032802\n",
            "Train Epoch: 4 [84224/120000 (83%)]\tLoss: 0.017964\n",
            "Train Epoch: 4 [87232/120000 (86%)]\tLoss: 0.054423\n",
            "Train Epoch: 4 [90240/120000 (88%)]\tLoss: 0.041358\n",
            "Train Epoch: 4 [93248/120000 (91%)]\tLoss: 0.046873\n",
            "Train Epoch: 4 [96256/120000 (94%)]\tLoss: 0.061877\n",
            "Train Epoch: 4 [99264/120000 (97%)]\tLoss: 0.008057\n",
            "\n",
            "Test set: Average loss: 0.0095, Accuracy: 17585/120000 (14.7%)\n",
            "\n",
            "Train Epoch: 5 [    0/120000 (0%)]\tLoss: 0.030836\n",
            "Train Epoch: 5 [ 3008/120000 (3%)]\tLoss: 0.003081\n",
            "Train Epoch: 5 [ 6016/120000 (6%)]\tLoss: 0.008989\n",
            "Train Epoch: 5 [ 9024/120000 (9%)]\tLoss: 0.012404\n",
            "Train Epoch: 5 [12032/120000 (12%)]\tLoss: 0.009837\n",
            "Train Epoch: 5 [15040/120000 (15%)]\tLoss: 0.024010\n",
            "Train Epoch: 5 [18048/120000 (18%)]\tLoss: 0.009363\n",
            "Train Epoch: 5 [21056/120000 (21%)]\tLoss: 0.005590\n",
            "Train Epoch: 5 [24064/120000 (24%)]\tLoss: 0.016381\n",
            "Train Epoch: 5 [27072/120000 (27%)]\tLoss: 0.051085\n",
            "Train Epoch: 5 [30080/120000 (29%)]\tLoss: 0.005245\n",
            "Train Epoch: 5 [33088/120000 (32%)]\tLoss: 0.011176\n",
            "Train Epoch: 5 [36096/120000 (35%)]\tLoss: 0.007610\n",
            "Train Epoch: 5 [39104/120000 (38%)]\tLoss: 0.016798\n",
            "Train Epoch: 5 [42112/120000 (41%)]\tLoss: 0.019578\n",
            "Train Epoch: 5 [45120/120000 (44%)]\tLoss: 0.024065\n",
            "Train Epoch: 5 [48128/120000 (47%)]\tLoss: 0.009840\n",
            "Train Epoch: 5 [51136/120000 (50%)]\tLoss: 0.040454\n",
            "Train Epoch: 5 [54144/120000 (53%)]\tLoss: 0.041623\n",
            "Train Epoch: 5 [57152/120000 (56%)]\tLoss: 0.009731\n",
            "Train Epoch: 5 [60160/120000 (59%)]\tLoss: 0.039213\n",
            "Train Epoch: 5 [63168/120000 (62%)]\tLoss: 0.005131\n",
            "Train Epoch: 5 [66176/120000 (65%)]\tLoss: 0.002757\n",
            "Train Epoch: 5 [69184/120000 (68%)]\tLoss: 0.001959\n",
            "Train Epoch: 5 [72192/120000 (71%)]\tLoss: 0.027733\n",
            "Train Epoch: 5 [75200/120000 (74%)]\tLoss: 0.019978\n",
            "Train Epoch: 5 [78208/120000 (77%)]\tLoss: 0.013056\n",
            "Train Epoch: 5 [81216/120000 (80%)]\tLoss: 0.070209\n",
            "Train Epoch: 5 [84224/120000 (83%)]\tLoss: 0.034952\n",
            "Train Epoch: 5 [87232/120000 (86%)]\tLoss: 0.004421\n",
            "Train Epoch: 5 [90240/120000 (88%)]\tLoss: 0.012522\n",
            "Train Epoch: 5 [93248/120000 (91%)]\tLoss: 0.010688\n",
            "Train Epoch: 5 [96256/120000 (94%)]\tLoss: 0.014567\n",
            "Train Epoch: 5 [99264/120000 (97%)]\tLoss: 0.048627\n",
            "\n",
            "Test set: Average loss: 0.0071, Accuracy: 17710/120000 (14.8%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ozgdyUjyNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import csv\n",
        "\n",
        "dir = \"/content/test/test\"\n",
        "files = os.listdir(dir)\n",
        "results = []\n",
        "for f in files:\n",
        "    image = Image.open(dir+\"/\"+f)\n",
        "    normalized_image = transform(image).unsqueeze(0).to(device)\n",
        "    prediction = model(normalized_image)\n",
        "    prediction = prediction[0]\n",
        "    probabilities = torch.softmax(prediction, -1)\n",
        "    probability = probabilities[1].detach().cpu().item()\n",
        "    results.append((f, probability))\n",
        "    \n",
        "with open('results.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"imagename\", \"is_fake\"])\n",
        "    for el in results:\n",
        "        writer.writerow([el[0], el[1]])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}