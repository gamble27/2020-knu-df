{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St3h5ubPLGKg",
        "colab_type": "text"
      },
      "source": [
        "## **Task**\n",
        "Train and improve model for CIFAR10 + show confusion matrix and recall, precision etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ5iTmBRC8gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6m_T4BidzaW",
        "colab_type": "text"
      },
      "source": [
        "## **Loading dataset**\n",
        "I added data augmentation!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMXGqrTjGc7s",
        "colab_type": "code",
        "outputId": "a3e7531c-596c-4fd6-9107-03a871ffa0c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "use_cuda = True\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_batch_size=64\n",
        "test_batch_size=1000\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(datasets.CIFAR10('data', train=True, download=True, transform=transform),\n",
        "                    batch_size=train_batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(datasets.CIFAR10('data', train=False, transform=transform),\n",
        "                    batch_size=test_batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "visualize_loader = torch.utils.data.DataLoader(datasets.CIFAR10('data', train=False, download=True, transform=transforms.Compose([transforms.ToTensor()])),\n",
        "                    batch_size=train_batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt9H9WhkeEiW",
        "colab_type": "text"
      },
      "source": [
        "## **Visualize some images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W5vgX27cq6a",
        "colab_type": "code",
        "outputId": "fe779905-f9b8-4d5d-ef1b-c960edb2da44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "# visualize data\n",
        "import seaborn as sn\n",
        "fig=plt.figure(figsize=(2, 2))\n",
        "data, label = next(iter(visualize_loader))\n",
        "print(label[11])\n",
        "img = data[11][0]\n",
        "plt.imshow(img)\n",
        "# for i in range(1, 51):\n",
        "#     img = data[i][0]\n",
        "#     fig.add_subplot(5, 10, i)\n",
        "#     plt.imshow(img)\n",
        "#     # img, vmin=0, vmax=1, cmap='gray'\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI4AAACOCAYAAADn/TAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGUIp6lphFqE",
        "colab_type": "text"
      },
      "source": [
        "You better not comment my green pictures. They are green, because you didn't sleep!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ht79x8CeS5Z",
        "colab_type": "text"
      },
      "source": [
        "## **Model definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_NOXTQ-GK6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 75, 5)\n",
        "        self.bnm1 = nn.BatchNorm2d(75)\n",
        "        self.conv2 = nn.Conv2d(75, 200, 5)\n",
        "        self.bnm2 = nn.BatchNorm2d(200)\n",
        "        self.conv3 = nn.Conv2d(200, 600, 3)\n",
        "        self.bnm3 = nn.BatchNorm2d(600)\n",
        "        self.conv4 = nn.Conv2d(600, 1200, 3)\n",
        "        self.bnm4 = nn.BatchNorm2d(1200)\n",
        "        self.conv5 = nn.Conv2d(1200, 2000, 3)\n",
        "        self.bnm5 = nn.BatchNorm2d(2000)\n",
        "        self.fc1 = nn.Linear(2*2*2000, 500)\n",
        "        self.fc2 = nn.Linear(500, 200)\n",
        "        self.fc3 = nn.Linear(200, 60)\n",
        "        self.fc4 = nn.Linear(60, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.bnm1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2) \n",
        "        x = self.bnm2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.bnm3(x)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.max_pool2d(x, 2, 2)       \n",
        "        x = self.bnm4(x)\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.bnm5(x)\n",
        "        x = x.view(-1, 2*2*2000)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6XgShoHetQ6",
        "colab_type": "text"
      },
      "source": [
        "## **Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5sZ6btaI3kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "lr=0.05\n",
        "momentum = 0.5\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_SbwNQNe3yT",
        "colab_type": "text"
      },
      "source": [
        "## **Functions for training and testing the model**\n",
        "They were taken from example with MNIST-FASHION dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDZo_fDcDOD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch,log_interval):\n",
        "    model.train()\n",
        "    avg_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad() # zero the gradient buffers\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step() # Does the update\n",
        "        avg_loss+=F.nll_loss(output, target, reduction='sum').item()\n",
        "        \n",
        "        if batch_idx % log_interval == 0:\n",
        "          print('Train Epoch: {} [{:5.0f}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\n",
        "    avg_loss/=len(train_loader.dataset)\n",
        "    return avg_loss\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    time.sleep(0.2)\n",
        "    return test_loss,accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEEDsNzFfRv4",
        "colab_type": "text"
      },
      "source": [
        "## **Training model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAEIpawOC_Wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 5\n",
        "log_interval = 600\n",
        "save_model = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ptLUxVgKPPh",
        "colab_type": "text"
      },
      "source": [
        "We break epochs into independent parts, where we change the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU3Hq2VUjiKN",
        "colab_type": "code",
        "outputId": "947aefcf-24a5-4d9d-f40d-2821aab44be4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "accuracy_list = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    trn_loss = train(model, device, train_loader, optimizer, epoch,log_interval)\n",
        "    test_loss,accuracy = test(model, device, test_loader)\n",
        "    train_losses.append(trn_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    accuracy_list.append(accuracy)\n",
        "    \n",
        "\n",
        "for i in range(0, 6):\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr/(i+1), momentum=momentum)\n",
        "  for epoch in range(epochs + i*5 + 1, epochs + i*5 + 6):\n",
        "      trn_loss = train(model, device, train_loader, optimizer, epoch,log_interval)\n",
        "      test_loss,accuracy = test(model, device, test_loader)\n",
        "      train_losses.append(trn_loss)\n",
        "      test_losses.append(test_loss)\n",
        "      accuracy_list.append(accuracy)\n",
        "epochs = epoch\n",
        "\n",
        "momentum = 0.1\n",
        "for i in range(1, 4):\n",
        "  lr = 0.001/i\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum*i)\n",
        "  for epoch in range(epochs + (i-1)*3 + 1, epochs + (i-1)*3 + 4):\n",
        "      trn_loss = train(model, device, train_loader, optimizer, epoch,log_interval)\n",
        "      test_loss,accuracy = test(model, device, test_loader)\n",
        "      train_losses.append(trn_loss)\n",
        "      test_losses.append(test_loss)\n",
        "      accuracy_list.append(accuracy)\n",
        "      \n",
        "epochs = epoch\n",
        "momentum = 0.01\n",
        "for i in range(1, 4):\n",
        "  lr = 0.0001/i\n",
        "  momentum *= i*i\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "  for epoch in range(epochs + (i-1)*3 + 1, epochs + (i-1)*3 + 4):\n",
        "      trn_loss = train(model, device, train_loader, optimizer, epoch,log_interval)\n",
        "      test_loss,accuracy = test(model, device, test_loader)\n",
        "      train_losses.append(trn_loss)\n",
        "      test_losses.append(test_loss)\n",
        "      accuracy_list.append(accuracy)\n",
        "\n",
        "torch.save(model.state_dict(),\"mnist_cnn.pt\") #saving"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [    0/50000 (0%)]\tLoss: 2.299377\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.050603\n",
            "\n",
            "Test set: Average loss: 1.2490, Accuracy: 5368/10000 (53.7%)\n",
            "\n",
            "Train Epoch: 2 [    0/50000 (0%)]\tLoss: 1.045251\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.055744\n",
            "\n",
            "Test set: Average loss: 0.9687, Accuracy: 6552/10000 (65.5%)\n",
            "\n",
            "Train Epoch: 3 [    0/50000 (0%)]\tLoss: 0.827433\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.861865\n",
            "\n",
            "Test set: Average loss: 0.8905, Accuracy: 6908/10000 (69.1%)\n",
            "\n",
            "Train Epoch: 4 [    0/50000 (0%)]\tLoss: 0.969620\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.752427\n",
            "\n",
            "Test set: Average loss: 0.7669, Accuracy: 7333/10000 (73.3%)\n",
            "\n",
            "Train Epoch: 5 [    0/50000 (0%)]\tLoss: 0.806832\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.696803\n",
            "\n",
            "Test set: Average loss: 0.7010, Accuracy: 7576/10000 (75.8%)\n",
            "\n",
            "Train Epoch: 6 [    0/50000 (0%)]\tLoss: 0.764991\n",
            "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.701730\n",
            "\n",
            "Test set: Average loss: 0.6957, Accuracy: 7644/10000 (76.4%)\n",
            "\n",
            "Train Epoch: 7 [    0/50000 (0%)]\tLoss: 0.739255\n",
            "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.653930\n",
            "\n",
            "Test set: Average loss: 0.6776, Accuracy: 7737/10000 (77.4%)\n",
            "\n",
            "Train Epoch: 8 [    0/50000 (0%)]\tLoss: 0.573954\n",
            "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.601186\n",
            "\n",
            "Test set: Average loss: 0.6050, Accuracy: 7974/10000 (79.7%)\n",
            "\n",
            "Train Epoch: 9 [    0/50000 (0%)]\tLoss: 0.449823\n",
            "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.652362\n",
            "\n",
            "Test set: Average loss: 0.5779, Accuracy: 8039/10000 (80.4%)\n",
            "\n",
            "Train Epoch: 10 [    0/50000 (0%)]\tLoss: 0.314995\n",
            "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.332282\n",
            "\n",
            "Test set: Average loss: 0.6042, Accuracy: 8052/10000 (80.5%)\n",
            "\n",
            "Train Epoch: 11 [    0/50000 (0%)]\tLoss: 0.562960\n",
            "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 0.461359\n",
            "\n",
            "Test set: Average loss: 0.4691, Accuracy: 8443/10000 (84.4%)\n",
            "\n",
            "Train Epoch: 12 [    0/50000 (0%)]\tLoss: 0.288704\n",
            "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 0.242947\n",
            "\n",
            "Test set: Average loss: 0.4384, Accuracy: 8516/10000 (85.2%)\n",
            "\n",
            "Train Epoch: 13 [    0/50000 (0%)]\tLoss: 0.259392\n",
            "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 0.262397\n",
            "\n",
            "Test set: Average loss: 0.4440, Accuracy: 8523/10000 (85.2%)\n",
            "\n",
            "Train Epoch: 14 [    0/50000 (0%)]\tLoss: 0.260050\n",
            "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 0.178183\n",
            "\n",
            "Test set: Average loss: 0.4544, Accuracy: 8501/10000 (85.0%)\n",
            "\n",
            "Train Epoch: 15 [    0/50000 (0%)]\tLoss: 0.306963\n",
            "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 0.244294\n",
            "\n",
            "Test set: Average loss: 0.4747, Accuracy: 8470/10000 (84.7%)\n",
            "\n",
            "Train Epoch: 16 [    0/50000 (0%)]\tLoss: 0.175145\n",
            "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 0.185007\n",
            "\n",
            "Test set: Average loss: 0.4412, Accuracy: 8619/10000 (86.2%)\n",
            "\n",
            "Train Epoch: 17 [    0/50000 (0%)]\tLoss: 0.076025\n",
            "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 0.222835\n",
            "\n",
            "Test set: Average loss: 0.4401, Accuracy: 8618/10000 (86.2%)\n",
            "\n",
            "Train Epoch: 18 [    0/50000 (0%)]\tLoss: 0.117422\n",
            "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 0.258682\n",
            "\n",
            "Test set: Average loss: 0.4397, Accuracy: 8660/10000 (86.6%)\n",
            "\n",
            "Train Epoch: 19 [    0/50000 (0%)]\tLoss: 0.283090\n",
            "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 0.414777\n",
            "\n",
            "Test set: Average loss: 0.4440, Accuracy: 8682/10000 (86.8%)\n",
            "\n",
            "Train Epoch: 20 [    0/50000 (0%)]\tLoss: 0.129791\n",
            "Train Epoch: 20 [38400/50000 (77%)]\tLoss: 0.174912\n",
            "\n",
            "Test set: Average loss: 0.4964, Accuracy: 8576/10000 (85.8%)\n",
            "\n",
            "Train Epoch: 21 [    0/50000 (0%)]\tLoss: 0.189971\n",
            "Train Epoch: 21 [38400/50000 (77%)]\tLoss: 0.148561\n",
            "\n",
            "Test set: Average loss: 0.4747, Accuracy: 8634/10000 (86.3%)\n",
            "\n",
            "Train Epoch: 22 [    0/50000 (0%)]\tLoss: 0.095262\n",
            "Train Epoch: 22 [38400/50000 (77%)]\tLoss: 0.209108\n",
            "\n",
            "Test set: Average loss: 0.4591, Accuracy: 8711/10000 (87.1%)\n",
            "\n",
            "Train Epoch: 23 [    0/50000 (0%)]\tLoss: 0.054453\n",
            "Train Epoch: 23 [38400/50000 (77%)]\tLoss: 0.022985\n",
            "\n",
            "Test set: Average loss: 0.4812, Accuracy: 8720/10000 (87.2%)\n",
            "\n",
            "Train Epoch: 24 [    0/50000 (0%)]\tLoss: 0.029865\n",
            "Train Epoch: 24 [38400/50000 (77%)]\tLoss: 0.264107\n",
            "\n",
            "Test set: Average loss: 0.4897, Accuracy: 8673/10000 (86.7%)\n",
            "\n",
            "Train Epoch: 25 [    0/50000 (0%)]\tLoss: 0.075931\n",
            "Train Epoch: 25 [38400/50000 (77%)]\tLoss: 0.063924\n",
            "\n",
            "Test set: Average loss: 0.4860, Accuracy: 8705/10000 (87.0%)\n",
            "\n",
            "Train Epoch: 26 [    0/50000 (0%)]\tLoss: 0.050825\n",
            "Train Epoch: 26 [38400/50000 (77%)]\tLoss: 0.202023\n",
            "\n",
            "Test set: Average loss: 0.4877, Accuracy: 8790/10000 (87.9%)\n",
            "\n",
            "Train Epoch: 27 [    0/50000 (0%)]\tLoss: 0.025676\n",
            "Train Epoch: 27 [38400/50000 (77%)]\tLoss: 0.038037\n",
            "\n",
            "Test set: Average loss: 0.5032, Accuracy: 8727/10000 (87.3%)\n",
            "\n",
            "Train Epoch: 28 [    0/50000 (0%)]\tLoss: 0.095538\n",
            "Train Epoch: 28 [38400/50000 (77%)]\tLoss: 0.087084\n",
            "\n",
            "Test set: Average loss: 0.5002, Accuracy: 8746/10000 (87.5%)\n",
            "\n",
            "Train Epoch: 29 [    0/50000 (0%)]\tLoss: 0.061282\n",
            "Train Epoch: 29 [38400/50000 (77%)]\tLoss: 0.089355\n",
            "\n",
            "Test set: Average loss: 0.5182, Accuracy: 8738/10000 (87.4%)\n",
            "\n",
            "Train Epoch: 30 [    0/50000 (0%)]\tLoss: 0.044629\n",
            "Train Epoch: 30 [38400/50000 (77%)]\tLoss: 0.097309\n",
            "\n",
            "Test set: Average loss: 0.4995, Accuracy: 8740/10000 (87.4%)\n",
            "\n",
            "Train Epoch: 31 [    0/50000 (0%)]\tLoss: 0.004695\n",
            "Train Epoch: 31 [38400/50000 (77%)]\tLoss: 0.057938\n",
            "\n",
            "Test set: Average loss: 0.5318, Accuracy: 8753/10000 (87.5%)\n",
            "\n",
            "Train Epoch: 32 [    0/50000 (0%)]\tLoss: 0.027885\n",
            "Train Epoch: 32 [38400/50000 (77%)]\tLoss: 0.016948\n",
            "\n",
            "Test set: Average loss: 0.5345, Accuracy: 8807/10000 (88.1%)\n",
            "\n",
            "Train Epoch: 33 [    0/50000 (0%)]\tLoss: 0.031919\n",
            "Train Epoch: 33 [38400/50000 (77%)]\tLoss: 0.033629\n",
            "\n",
            "Test set: Average loss: 0.5328, Accuracy: 8760/10000 (87.6%)\n",
            "\n",
            "Train Epoch: 34 [    0/50000 (0%)]\tLoss: 0.033980\n",
            "Train Epoch: 34 [38400/50000 (77%)]\tLoss: 0.062518\n",
            "\n",
            "Test set: Average loss: 0.5353, Accuracy: 8803/10000 (88.0%)\n",
            "\n",
            "Train Epoch: 35 [    0/50000 (0%)]\tLoss: 0.084575\n",
            "Train Epoch: 35 [38400/50000 (77%)]\tLoss: 0.046897\n",
            "\n",
            "Test set: Average loss: 0.5468, Accuracy: 8784/10000 (87.8%)\n",
            "\n",
            "Train Epoch: 36 [    0/50000 (0%)]\tLoss: 0.040216\n",
            "Train Epoch: 36 [38400/50000 (77%)]\tLoss: 0.001875\n",
            "\n",
            "Test set: Average loss: 0.5422, Accuracy: 8768/10000 (87.7%)\n",
            "\n",
            "Train Epoch: 37 [    0/50000 (0%)]\tLoss: 0.009092\n",
            "Train Epoch: 37 [38400/50000 (77%)]\tLoss: 0.013545\n",
            "\n",
            "Test set: Average loss: 0.5262, Accuracy: 8814/10000 (88.1%)\n",
            "\n",
            "Train Epoch: 38 [    0/50000 (0%)]\tLoss: 0.014517\n",
            "Train Epoch: 38 [38400/50000 (77%)]\tLoss: 0.014364\n",
            "\n",
            "Test set: Average loss: 0.5273, Accuracy: 8808/10000 (88.1%)\n",
            "\n",
            "Train Epoch: 39 [    0/50000 (0%)]\tLoss: 0.014247\n",
            "Train Epoch: 39 [38400/50000 (77%)]\tLoss: 0.103902\n",
            "\n",
            "Test set: Average loss: 0.5166, Accuracy: 8837/10000 (88.4%)\n",
            "\n",
            "Train Epoch: 40 [    0/50000 (0%)]\tLoss: 0.010633\n",
            "Train Epoch: 40 [38400/50000 (77%)]\tLoss: 0.023163\n",
            "Train Epoch: 41 [    0/50000 (0%)]\tLoss: 0.046657\n",
            "Train Epoch: 41 [38400/50000 (77%)]\tLoss: 0.084358\n",
            "\n",
            "Test set: Average loss: 0.5321, Accuracy: 8854/10000 (88.5%)\n",
            "\n",
            "Train Epoch: 42 [    0/50000 (0%)]\tLoss: 0.027778\n",
            "Train Epoch: 42 [38400/50000 (77%)]\tLoss: 0.008792\n",
            "\n",
            "Test set: Average loss: 0.5311, Accuracy: 8825/10000 (88.2%)\n",
            "\n",
            "Train Epoch: 43 [    0/50000 (0%)]\tLoss: 0.005926\n",
            "Train Epoch: 43 [38400/50000 (77%)]\tLoss: 0.067160\n",
            "Train Epoch: 44 [    0/50000 (0%)]\tLoss: 0.013754\n",
            "Train Epoch: 44 [38400/50000 (77%)]\tLoss: 0.005312\n",
            "\n",
            "Test set: Average loss: 0.5216, Accuracy: 8855/10000 (88.5%)\n",
            "\n",
            "Train Epoch: 44 [    0/50000 (0%)]\tLoss: 0.006125\n",
            "\n",
            "Test set: Average loss: 0.5797, Accuracy: 8121/10000 (81.2%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYunNV_lXTQR",
        "colab_type": "text"
      },
      "source": [
        "Making confusion Matrix with sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6jbAaAWXXqw",
        "colab_type": "code",
        "outputId": "206d8982-4d90-44c2-804a-dabf9796c8bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(1000):\n",
        "            label = labels[i]\n",
        "            y_true.append(classes[labels[i]])\n",
        "            y_pred.append(classes[predicted[i]])\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "import sklearn.metrics as metr\n",
        "mtrx = metr.confusion_matrix(y_true, y_pred, labels=classes)\n",
        "print(\"Confusion matrix: \\n\", mtrx)\n",
        "\n",
        "precisions = [0,0,0,0,0,0,0,0,0,0]\n",
        "recalls = [0,0,0,0,0,0,0,0,0,0]\n",
        "f1_scores = [0,0,0,0,0,0,0,0,0,0]\n",
        "for i in range(10):\n",
        "    suma_row=0\n",
        "    suma_column=0\n",
        "    for j in range(10):\n",
        "        suma_row += mtrx[i][j]\n",
        "        suma_column += mtrx[j][i]\n",
        "    precisions[i] = mtrx[i][i]/(suma_row)*100\n",
        "    recalls[i] = mtrx[i][i]/(suma_column)*100\n",
        "    f1_scores[i] = 2 * (precisions[i] * recalls[i])/(precisions[i] + recalls[i])\n",
        "    print('Accuracy of %5s : %2d %%  Precision: %4s %%  Recall: %4d %%  F1-score: %4s %%' % (classes[i], 100 * class_correct[i] / class_total[i], precisions[i], recalls[i], f1_scores[i]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix: \n",
            " [[855   9  26  12   8   0   5   7  51  27]\n",
            " [ 19 873   3   1   1   0   3   2  37  61]\n",
            " [ 52   9 759  28  48  20  34  28  11  11]\n",
            " [ 22   4  74 632  60 107  32  32  17  20]\n",
            " [ 14   3  45  21 812  10  18  47  21   9]\n",
            " [  9   7  44 120  45 701  11  44   4  15]\n",
            " [  9   4  32  31  29  10 867   5   6   7]\n",
            " [ 15  10  17  29  27  21   2 852   5  22]\n",
            " [ 41   8   7   7   0   0   3   4 883  47]\n",
            " [ 17  35   4   6   0   1   3   3  22 909]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}